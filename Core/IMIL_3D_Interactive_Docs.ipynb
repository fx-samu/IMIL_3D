{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c707dc24",
   "metadata": {},
   "source": [
    "# IMIL-3D: Image Manipulation Interpretation Lab 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeb43dc",
   "metadata": {},
   "source": [
    "> _A Python framework for converting single 2D images into textured 3D mesh reconstructions._\n",
    "\n",
    "Welcome to 3D IMIL a lab created by B.Sc. Candidate Samuel Chavez F. in under supervision by PhD. Nelson Baloian and inspired and direction of PhD. Candidate Cristian Llull, FCFM, Universidad de Chile.\n",
    "\n",
    "The module lab presents the foundations of working with single images in Python to streamline the process of interpreting a static image and infere depth to recreate good fidelity 3D estimations of the object using different methods initially in the scope of culture presevation and digitalization of Khachkars\n",
    "\n",
    "Next is presented a interactive documentation of what you can find in this lab, in conjuction with this docs there is main as example pipeline end to end that uses the methods to get a object reconstruction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78d1638",
   "metadata": {},
   "source": [
    "<div class=\"centered\">\n",
    "    <img src=\"../Assets/Khachckars/1.jpg\" width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caff612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - Run this cell first\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imil_3d as im3\n",
    "\n",
    "# Configuration\n",
    "IMG_PATH = \"../Assets/Khachckars/1.jpg\"\n",
    "IMG_RES = 0.1  # Megapixels (lower = faster for demos)\n",
    "\n",
    "# SAM2 paths (adjust if needed)\n",
    "CHECKPOINT_DIR = \"../checkpoints\"\n",
    "SAM2_PT = f\"{CHECKPOINT_DIR}/sam2.1_hiera_base_plus.pt\"\n",
    "SAM2_CFG = \"configs/sam2.1/sam2.1_hiera_b+.yaml\"\n",
    "\n",
    "# Load example image\n",
    "img_arr = im3.functions.image_open(IMG_PATH)\n",
    "img_arr = im3.field_manipulation.image_resize(img_arr, mp=IMG_RES)\n",
    "\n",
    "plt.figure(figsize=(6, 8))\n",
    "plt.imshow(img_arr)\n",
    "plt.title(f\"Example Image: Khachkar 1 ({img_arr.shape[1]}x{img_arr.shape[0]})\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Image loaded: {img_arr.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff8d9e8",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719fb0b4",
   "metadata": {},
   "source": [
    "## Imiltyping: Backbone of contracts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48eac94",
   "metadata": {},
   "source": [
    "The `imiltyping` module defines type aliases using `nptyping` for strong typing throughout the codebase:\n",
    "\n",
    "| Type | Description |\n",
    "|------|-------------|\n",
    "| `ListPoint2D` | Array of 2D points `(N, 2)` |\n",
    "| `BoolField2D` | 2D boolean mask `(H, W)` |\n",
    "| `NumberField2D` | 2D numeric array `(H, W)` |\n",
    "| `ScalarField2D` | 2D array of any type `(H, W)` |\n",
    "| `VectorField3D` | 3D vector field `(H, W, 3)` |\n",
    "| `NumerPoint3D` | 3D point `(3,)` |\n",
    "| `GrayImageLikeArray` | Grayscale image `(H, W, 1)` |\n",
    "| `LAImageLikeArray` | Luminance + Alpha image `(H, W, 2)` |\n",
    "| `RGBImageLikeArray` | RGB image `(H, W, 3)` |\n",
    "| `RGBAImageLikeArray` | RGBA image `(H, W, 4)` |\n",
    "| `ImageLikeArray` | Union of all image types above |\n",
    "\n",
    "These types enable IDE autocompletion and runtime validation. And are key to complete the contracts that make this Lab functional and flexible to make it a pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8d74ac",
   "metadata": {},
   "source": [
    "## Field Recognition: Identifying in an image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351225d3",
   "metadata": {},
   "source": [
    "This module handles object detection and segmentation:\n",
    "\n",
    "### Key Functions:\n",
    "- **`sam_mask()` / `sam2_mask()`** - Segment objects using SAM/SAM2 with a bounding box\n",
    "- **`corners_rect()`** - Find corners via minimum area rectangle fitting\n",
    "- **`corners_hough()`** - Find corners using Hough line transform\n",
    "- **`corners_rdp()`** - Find corners using Ramer-Douglas-Peucker algorithm\n",
    "\n",
    "### Utility Functions:\n",
    "- **`best_wh_quad(corners)`** - Compute minimum width and height from 4 corner points\n",
    "- **`map_corners(corners)`** - Reorder 4 corners to CCW starting from top-left\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8ac5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Field Recognition Example: SAM2 Segmentation + Corner Detection\n",
    "\n",
    "# Get bounding box for image 1 at 0.5MP\n",
    "bbox = im3.field_recognition.magic_bounding_box(1, IMG_RES)\n",
    "print(f\"Bounding box: {bbox}\")\n",
    "\n",
    "# Segment with SAM2 (requires GPU)\n",
    "mask = im3.field_recognition.sam2_mask(img_arr, bbox, SAM2_PT, SAM2_CFG)\n",
    "\n",
    "# Apply mask to image\n",
    "img_masked = im3.field_manipulation.field_mask(img_arr, mask)\n",
    "\n",
    "# Detect corners using three different methods\n",
    "corners_rect = im3.field_recognition.corners_rect(img_masked)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].imshow(img_arr)\n",
    "axes[0].set_title(\"Original\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(mask, cmap='gray')\n",
    "axes[1].set_title(\"SAM2 Mask\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(img_masked)\n",
    "# Draw detected corners\n",
    "for i, corner in enumerate(corners_rect):\n",
    "    axes[2].plot(corner[0], corner[1], 'ro', markersize=10)\n",
    "    axes[2].annotate(str(i), (corner[0]+5, corner[1]+5), color='red', fontsize=12)\n",
    "axes[2].set_title(\"Masked + Corners (MinAreaRect)\")\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Detected corners:\\n{corners_rect}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92e3b99",
   "metadata": {},
   "source": [
    "## Field Manipulation: Modifying that Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc38f2ad",
   "metadata": {},
   "source": [
    "Image processing utilities for preparing data:\n",
    "\n",
    "### Key Functions:\n",
    "- **`image_resize(img, mp)`** - Resize to target megapixels\n",
    "- **`field_mask(img, mask)`** - Apply boolean mask to image\n",
    "- **`unwarp(img, corners)`** - Perspective correction using 4 corners\n",
    "- **`map_unwarp(img, corners)`** - Auto-orient corners + unwarp\n",
    "- **`field_normalization(field)`** - Normalize values to [0, 1]\n",
    "- **`field_normalization_mask(field, mask)`** - Normalize only masked region\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af64a3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Field Manipulation Example: Perspective Unwarp\n",
    "\n",
    "# Unwarp the masked image using detected corners\n",
    "unwarp_img = im3.field_manipulation.map_unwarp(img_masked, corners_rect)\n",
    "\n",
    "# Visualize the transformation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "axes[0].imshow(img_masked)\n",
    "axes[0].set_title(\"Masked Image (perspective)\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(unwarp_img)\n",
    "axes[1].set_title(f\"Unwarped ({unwarp_img.shape[1]}x{unwarp_img.shape[0]})\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Unwarped shape: {unwarp_img.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29b9b9a",
   "metadata": {},
   "source": [
    "## Height Map: Inference of depth from an Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4683d5c",
   "metadata": {},
   "source": [
    "Simple depth estimation from image luminance:\n",
    "\n",
    "### Key Functions:\n",
    "- **`grayscale(img)`** - Convert to grayscale with autocontrast as depth proxy\n",
    "\n",
    "*Note: This is a simplified approach. For better results, consider using neural depth estimation models like Depth Anything or MiDaS.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12e3fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Height Map Example: Generate depth from grayscale\n",
    "\n",
    "# Convert to height map\n",
    "height_map = im3.height_map.grayscale(unwarp_img)\n",
    "final_mask = height_map != 0\n",
    "\n",
    "# Normalize the height map within the mask\n",
    "norm_hmap = im3.field_manipulation.field_normalization_mask(height_map, final_mask)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].imshow(unwarp_img)\n",
    "axes[0].set_title(\"Unwarped Image\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(height_map, cmap='gray')\n",
    "axes[1].set_title(\"Height Map (grayscale)\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "im = axes[2].imshow(norm_hmap, cmap='viridis')\n",
    "axes[2].set_title(\"Normalized Height Map\")\n",
    "axes[2].axis('off')\n",
    "plt.colorbar(im, ax=axes[2], fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Height map range: [{norm_hmap.min():.3f}, {norm_hmap.max():.3f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f495bc",
   "metadata": {},
   "source": [
    "## Mesh Works: Making it real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b5ef03",
   "metadata": {},
   "source": [
    "Convert height maps to textured 3D meshes:\n",
    "\n",
    "### Key Functions:\n",
    "- **`mesh_hmap(height_map)`** - Create triangulated mesh from 2D height map\n",
    "- **`face_mask_from_vertex_mask(mesh, mask)`** - Convert vertex mask to face mask\n",
    "- **`mask_face_submesh(mesh, face_mask)`** - Extract submesh with only masked faces\n",
    "- **`solidify_mesh_box(mesh, height)`** - Extrude mesh into solid box\n",
    "- **`apply_texture_to_solid(mesh, image, color)`** - Apply UV-mapped texture\n",
    "- **`mesh_export(mesh, path)`** - Export to STL, OBJ, GLB, etc.\n",
    "\n",
    "### Utility Functions:\n",
    "- **`mask_vertex_mesh(mesh, mask)`** - Remove vertices from mesh based on a 2D boolean mask\n",
    "- **`face_mask_by_sampling(mesh, mask, width_ratio)`** - Create face mask by sampling vertex mask at face centers\n",
    "- **`pixel_contour(mask)`** - Detect corner points along the boundary of a binary mask\n",
    "- **`mesh_contour_mask(contour_points, mask)`** - Create a flat mesh from contour corner points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac26346e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mesh Works Example: Full 3D Reconstruction Pipeline\n",
    "\n",
    "# Step 1: Create mesh from height map\n",
    "DEPTH_SCALE = 0.05  # Adjust relief depth\n",
    "raw_mesh = im3.mesh_works.mesh_hmap(norm_hmap * DEPTH_SCALE)\n",
    "print(f\"Raw mesh: {len(raw_mesh.vertices)} vertices, {len(raw_mesh.faces)} faces\")\n",
    "\n",
    "# Step 2: Mask faces to object boundary\n",
    "f_mask_map = im3.mesh_works.face_mask_from_vertex_mask(raw_mesh, final_mask)\n",
    "front_mesh = im3.mesh_works.mask_face_submesh(raw_mesh, f_mask_map)\n",
    "print(f\"Masked mesh: {len(front_mesh.vertices)} vertices, {len(front_mesh.faces)} faces\")\n",
    "\n",
    "# Step 3: Solidify into a box\n",
    "BOX_DEPTH = 0.1\n",
    "solid_mesh = im3.mesh_works.solidify_mesh_box(front_mesh, BOX_DEPTH)\n",
    "print(f\"Solid mesh: {len(solid_mesh.vertices)} vertices, {len(solid_mesh.faces)} faces\")\n",
    "\n",
    "# Step 4: Apply texture\n",
    "mean_color = unwarp_img[final_mask].mean(axis=0).astype(np.uint8)\n",
    "textured_mesh = im3.mesh_works.apply_texture_to_solid(solid_mesh, unwarp_img, mean_color)\n",
    "print(f\"Textured mesh ready!\")\n",
    "\n",
    "# Step 5: Export\n",
    "im3.mesh_works.mesh_export(textured_mesh, \"../Assets/Models/example_output.glb\")\n",
    "print(f\"\\n✓ Exported to example_output.glb\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43db4585",
   "metadata": {},
   "source": [
    "# Extras & Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb562b82",
   "metadata": {},
   "source": [
    "## Functions: im3 to work within the lab framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e367bd94",
   "metadata": {},
   "source": [
    "Helper utilities for image I/O and benchmarking:\n",
    "\n",
    "### Key Functions:\n",
    "- **`image_open(path)`** - Load image as numpy array\n",
    "- **`image_from_imagearr(arr)`** - Convert array to PIL Image\n",
    "- **`imagearr_from_image(img)`** - Convert PIL Image to array\n",
    "- **`@benchmark`** - Decorator to measure execution time and memory\n",
    "- **`@gpu_benchmark`** - Decorator with GPU memory tracking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3850570",
   "metadata": {},
   "source": [
    "## Texture Crafting: A refined way to give it life to the object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29881195",
   "metadata": {},
   "source": [
    "*Work in Progress* - Advanced texture manipulation utilities for texture enhancing with more ingrained detail manipulation of them that enhance the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf48bce",
   "metadata": {},
   "source": [
    "# Complete Pipeline Summary example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f8a6cb",
   "metadata": {},
   "source": [
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                    3D-IMIL Pipeline                             │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│  1. Load Image         →  functions.image_open()                │\n",
    "│  2. Resize             →  field_manipulation.image_resize()     │\n",
    "│  3. Segment Object     →  field_recognition.sam2_mask()         │\n",
    "│  4. Apply Mask         →  field_manipulation.field_mask()       │\n",
    "│  5. Detect Corners     →  field_recognition.corners_*()         │\n",
    "│  6. Unwarp Perspective →  field_manipulation.map_unwarp()       │\n",
    "│  7. Generate Height    →  height_map.grayscale()                │\n",
    "│  8. Normalize          →  field_manipulation.field_normalization│\n",
    "│  9. Create Mesh        →  mesh_works.mesh_hmap()                │\n",
    "│ 10. Mask Mesh          →  mesh_works.mask_face_submesh()        │\n",
    "│ 11. Solidify           →  mesh_works.solidify_mesh_box()        │\n",
    "│ 12. Apply Texture      →  mesh_works.apply_texture_to_solid()   │\n",
    "│ 13. Export             →  mesh_works.mesh_export()              │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "See `main.py` for a complete end-to-end implementation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
